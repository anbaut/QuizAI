import express from "express";
import fetch from "node-fetch";
import cors from "cors";

const app = express();
app.use(cors());
app.use(express.json());

// ðŸš¨ VÃ©rifie bien le nom du modÃ¨le dans LM Studio
const LM_MODEL = "mistral-7b-instruct-v0.3"; // ex: "mistral-7b-instruct"

const LM_URL = "http://localhost:1234/v1/chat/completions";

app.post("/api/question", async (req, res) => {
  const { category, difficulty } = req.body;

  const prompt = `
Tu es un gÃ©nÃ©rateur de questions pour un jeu.

CatÃ©gorie : ${category}
DifficultÃ© : ${difficulty}

RÃ¨gles :
- Une seule question
- Langue : franÃ§ais
- Si difficultÃ© = Facile ou Moyen â†’ QCM
- Si difficultÃ© = Difficile â†’ rÃ©ponse libre
- Pas d'explications
- Pas de texte autour du JSON

RÃ©ponds STRICTEMENT au format JSON suivant :

{
  "question": "string",
  "type": "qcm" ou "text",
  "choices": ["string"],
  "answer": "string"
}
`;

  try {
    const response = await fetch(LM_URL, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: LM_MODEL,
        messages: [
          { role: "system", content: "Tu es un assistant de jeu qui gÃ©nÃ¨re des questions." },
          { role: "user", content: prompt }
        ],
        temperature: 0.8
      })
    });

    const data = await response.json();

    // âœ… SÃ©curisation JSON : on retire les ```json ``` Ã©ventuels
    let content = data.choices?.[0]?.message?.content || "";
    content = content.replace(/```json/g, "").replace(/```/g, "").trim();

    const parsed = JSON.parse(content);
    res.json(parsed);

  } catch (err) {
    console.error("âŒ Erreur IA :", err);
    res.status(500).json({ error: "Erreur gÃ©nÃ©ration IA" });
  }
});

app.listen(3000, () => {
  console.log("âœ… Backend LM Studio prÃªt sur http://localhost:3000");
});
